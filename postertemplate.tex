\documentclass[final]{beamer}
\begin{filecontents}{postertemplate.bib}

\end{filecontents}
\usetheme{PSI}
\usepackage{natbib}
\usepackage{ragged2e}   % package for justifying
%\usepackage[demo]{graphicx}
\usepackage{caption}
%\usepackage{subcaption}
%\usepackage[caption=false]{subfig}

\addtobeamertemplate{block begin}{}{\justifying}  %new code
\bibliographystyle{abbrv}
\usepackage[orientation=portrait,size=a0,scale=1.4,debug]{beamerposter}
\usepackage[absolute,overlay]{textpos}

\setlength{\TPHorizModule}{1cm}
\setlength{\TPVertModule}{1cm}

\captionsetup[figure]{labelformat=simple}
\captionsetup[table]{labelformat=simple}

\title{Voice Control for a Gripper Using Mel-Frequency Cepstral Coefficients and Gaussian Mixture Models}
\author{Velasco-Hernandez, Gustavo $^{1}$. D\'{i}az-Toro, Andr\'{e}s Alejandro $^{2}$}
\institute{Perception and Intelligent Systems Research Group\\
	School of Electric and Electronics Engineering\\
	Universidad del Valle}
\footer{Contact:\\ 1. velasco.gustavo@correounivalle.edu.co\\ 2. andres.a.diaz@correounivalle.edu.co}
\date{}

\hyphenation{performance}
\begin{document}
\begin{frame}{}

\begin{textblock}{5}(3,3)
\includegraphics[width=0.10\paperwidth]{figs/univalle.png}
\end{textblock}
\begin{textblock}{5}(70,5.5)
\includegraphics[width=0.15\paperwidth]{figs/psilogo3.png}
\end{textblock}


\begin{textblock}{39.5}(1,19)
\begin{block}{Abstract}
This work presents an implementation of a speaker-dependent speech recognition system used to control a gripper. The application was made using MATLAB and the gripper was assembled using the Lego Mindstorm NXT robotic kit. Four commands are implemented for controlling the gripper: Open, close, rotate left and rotate right. The development was divided into two stages. In training stage, we use Mel Frequency Cepstral Coefficients (MFCCs) and Gaussian Mixture Models (GMMs) to generate a representation of each defined command. Then, in testing stage, those models are used to identify the speakerâ€™s utterance and send the command to the actuator. Finally, we present test results that show a performance of 95.09\% for our system, and then we compare it with similar works.
\end{block}

\begin{block}{Methodology}

The methodology developed along this work is based on two main features. The first one is that the system is focused on detecting isolated words; this means that the speaker will say words separated by silence spaces. The second feature is that the system detects words only for the speaker for whom the models were computed. The development of this system was divided into training stage (figure \ref{trainingBlocks}) and development stage (figure \ref{testingBlocks}). Training stage is intended to build acoustic model based on the utterances of speakers. Testing stage capture speech signal and use previously created models to validate it and execute commands.

\begin{columns}[t]
\column{.5\textwidth}
\centering
\begin{figure}
	\includegraphics[width=\linewidth]{figs/1bloquesTraining1}
    \caption{Training Stage}
	\label{trainingBlocks}
\end{figure}
\column{.5\textwidth}
\centering
\begin{figure}
    \includegraphics[width=\linewidth]{figs/11bloquesTesting1}
    \caption{Testing Stage.}
	\label{testingBlocks}
\end{figure}
\end{columns}

\vspace{1ex}

In training stage, after applying a high-pass filter (Pre-emphasis), the signal is split in sets of 160 samples and an energy analysis is performed in order to detect intervals where a word exists. Then, Mel Frequency Cepstral Coefficients are computed and a space vector of MFCC are obtained, and the final step is to estimate a multidimensional probability density function, which will be the acoustic model for a word and a speaker. This model is generated based on Gaussian Mixture Models, using eight gaussians.

Testing stage performs the same steps described for training stage, from pre-emphasis to MFCC computation. The calculated coefficients vector is used as input to pattern classification step. The output parameters of this step are the posterior probability and the negative likelihood of the data given the model. This process is based on equations \ref{prob} and \ref{prob2}, where $v$ is the coefficients vector, $\lambda$ is the model, $w_i$ are weights and $N$ corresponds to normal distribution with mean $\mu$ and variance $\sum$. $M$ is the number of mixtures. The \textit{posterior} function of Matlab estimates the index $\hat{J}$ of the model that maximizes the conditional probability.

\begin{equation}
   p(v \lvert \lambda)=\sum_{i=1}^{M}w_iN(v \lvert \mu_i,\sum_i) \label{prob}
\end{equation}

\begin{equation}
   \hat{J}= max_{j} p(v \lvert \lambda_j) \label{prob2}
\end{equation}

\vspace{1ex}

Finally, if there is a successfull detection of a word matching a model, a command is sent to the gripper.

\end{block}
\end{textblock}

\begin{textblock}{39.5}(43.05,19)

\begin{block}{Results}

Table \ref{performance} shows the performance of the system for two speakers and for changes in the number of Gaussians. The analysis was developed with regard to each command (command performance) and finally with regard to all the commands (global performance).

\begin{table}[h]
\centering
%\tiny
%\scriptsize
\footnotesize
%\small
\begin{tabular}{||p{3.5cm}||p{4cm}||p{3cm}||p{4cm}||p{4cm}||p{4cm}||p{4cm}||p{6cm}||}%p{3cm}
\hline 
\textbf{Speaker} &  \textbf{GMM Number} & \textbf{No. of words} & \multicolumn{4}{|c||}{\textbf{Command Performance (\%)}} & \textbf{Global Performance (\%)}\\
\cline{4-7}
& & & \textbf{Open} & \textbf{Close} & \textbf{Left} & \textbf{Right} & \\
\hline \hline

1 &5 &17 &94.11 &100 &94.11 &100 & \textbf{97.05} \\
\hline 
1 &8 &17 &100 &88.23 &82.35 &94.11 &\textbf{88.23} \\
\hline
1 &11 &17 &94.11 &82.35 &82.35 &94.11 &\textbf{88.23} \\
\hline
2 &5 &18 &100 &94.11 &94.11 &100 & \textbf{97.05}\\
\hline
2 &8 &16 &100 &100 &100 &100 & \textbf{100}\\
\hline
2 &11 &18 &100 &100 &100 &100 & \textbf{100}\\
\hline

\end{tabular}
\caption{Performance of the Voice Recognition System}
\label{performance}
\end{table}

In table \ref{table_comp} different approaches of speech recognition systems for voice commands are compared. The accuracy is over 85\% in all the solutions. Our system, with 95.09\%, is above the mean of the compared implementations.

\begin{table}[!ht]
	\small
	\begin{center}	
	\begin{tabular}{|c | p{19.5cm} | c|}
	\hline
	\textbf{Approach} & \textbf{Description} & \textbf{Performance} \\
	\hline
	\hline
	Tezer \cite{Tezer} &
		Used LPC and DTW. Implemented in MATLAB. &
		86\% \\
	Beritelli \cite{Beritelli} &
		Used VQ. Proposed to noise-robust application &
		~92\% \\
	Bedoya \cite{Bedoya} &
		Used wavelts, HMMs and MFCCs &
		98\% \\
	Phokharatkul \cite{Phokharatkul} &
		Used filter banks and Mel scale analysis &
		96.3\% \\
	Ali \cite{Ali} &
		Used MFCCs and ANN. Isolated or continuous speech mode &
		~96\% \\
	Chin \cite{Chin} &
		Used MFCCs and ANN&
		98.9\% \\
	Ours &
		Used MFFCs and GMMs, implemented in MATLAB &
		95.09\%\\
	\hline
	\end{tabular}
	\end{center}	
	\caption{Comparision of different speech recognition systems}
	\label{table_comp}
\end{table}

\end{block}

\begin{block}{Conclusions}
In this paper we presented an speaker-dependent speech recognition system based on Mel Frequency Cepstral Coefficients (MFCCs) for extracting features and Gaussian Mixture Models (GMMs) for creating the model of each command. We test the systems with two different speakers and the worst case was $91.17\%$ (average of global performance for speaker $1$) of accuracy for a speaker. For a particular command, the worst case was $82.35\%$ and the average of the global performance of the system was $95.09\%$ (see Results section).

As future work, we propose the evaluation of the system with more than four commands and the creation of models with utterances from different speakers in order to test the capability of the system to be speaker-independent.
\end{block}

\begin{block}{References}
\small
\bibliography{postertemplate.bib}
\end{block}

\end{textblock}

\end{frame}
\end{document}
